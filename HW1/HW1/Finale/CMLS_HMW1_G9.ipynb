{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Antonacci_HMW1_G9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdbeI4YRWWYH"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import scipy as sp\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import scipy.stats\n",
        "import seaborn as sns\n",
        "import os\n",
        "import sklearn.svm\n",
        "from google.colab import drive\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHUaXrNNnj_e",
        "outputId": "d784349f-604b-4702-9999-e90599f2f529"
      },
      "source": [
        "%cd sample_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'sample_data'\n",
            "/content/sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG9U88rDXWQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e9a457-65f7-4f7a-b2ef-6a1cf262f206"
      },
      "source": [
        "! git clone https://github.com/Jakobovski/free-spoken-digit-dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'free-spoken-digit-dataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBeKvQ5dlZW4"
      },
      "source": [
        "train_root = ('free-spoken-digit-dataset/recordings')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ozLpmSp7fVA"
      },
      "source": [
        "#Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAEGL-ZT7iNW"
      },
      "source": [
        "#mel frequency cepstrum\n",
        "def compute_mfcc(audio, fs, n_mfcc):\n",
        "    #Abs of stft\n",
        "    X = np.abs(librosa.stft(audio,\n",
        "                            window='hamming',\n",
        "                            n_fft = 512, \n",
        "                            hop_length= 256,\n",
        "                           ))\n",
        "    #mel\n",
        "    mel = librosa.filters.mel(\n",
        "        sr=fs,\n",
        "        n_fft = 512,\n",
        "        n_mels = 40,\n",
        "        fmin = 133.33,\n",
        "        fmax = 4000\n",
        "    )\n",
        "    melspectrogram = np.dot(mel, X) #filtering\n",
        "    log_melspectrogram = np.log10(melspectrogram +1e-16)\n",
        "    \n",
        "    mfcc = sp.fftpack.dct(log_melspectrogram, axis = 0, norm='ortho')[1:n_mfcc+ 1]\n",
        "    \n",
        "    \n",
        "    return mfcc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqxg-cLqwl2i"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK1SG0ASwuRq"
      },
      "source": [
        "classes = [0,1,2,3,4,5,6,7,8,9]\n",
        "n_mfcc = 13\n",
        "\n",
        "#\n",
        "dict_train_features = {0: [], 1: [], 2: [],3: [],4: [],5: [],6: [],7: [],8: [],9:[]}\n",
        "dict_test_features = {0: [], 1: [], 2: [],3: [],4: [],5: [],6: [],7: [],8: [],9:[]}\n",
        "\n",
        "#name of files\n",
        "class_train_files = [f for f in os.listdir(train_root) if f.endswith('.wav')]\n",
        "\n",
        "for i in np.arange(len(class_train_files)):\n",
        "  tmp = class_train_files[i];\n",
        "  tmp = (tmp.split('.'))[0].split('_');\n",
        "  \n",
        "  #open file\n",
        "  audio, fs = librosa.load(os.path.join(train_root,class_train_files[i]), sr=None)\n",
        "  \n",
        "  mfcc = compute_mfcc(audio, fs, n_mfcc)\n",
        "  tmp_features = np.mean(mfcc, axis=1);\n",
        "  \n",
        "  #if last index >4 ==> Train file\n",
        "  if int(tmp[2]) > 4:\n",
        "    dict_train_features[int(tmp[0])].append(tmp_features)\n",
        "  #if last index <4 ==> Test file\n",
        "  else:\n",
        "    dict_test_features[int(tmp[0])].append(tmp_features)\n",
        "\n",
        "Len_train = len(dict_train_features[0])\n",
        "Len_test = len(dict_test_features[0])\n",
        "Len_digit = 10;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWxMoFrN_o9f"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwPO5r2ZqUjd"
      },
      "source": [
        "We first initialize a 2-D matrix with all the values of the MFCC for all the train/test files -> 2700 x 13"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIlRklLCsQfn"
      },
      "source": [
        "y_train_dict = {0: [], 1: [], 2: [],3: [],4: [],5: [],6: [],7: [],8: [],9:[]}\n",
        "\n",
        "#initialize X-train 2-D matrix via an array \n",
        "X_train = np.array(dict_train_features[0])\n",
        "y_train_dict[0] = np.zeros((np.array(dict_train_features[0]).shape[0],))\n",
        "\n",
        "#iteratively concatenate the coloumns relative to the other digits\n",
        "for i in np.arange(1, Len_digit):\n",
        "    X_train = np.concatenate((X_train, dict_train_features[i]), axis = 0)\n",
        "    y_train_dict[i] = np.ones((np.array(dict_train_features[i]).shape[0],))*i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv2AQTpOsTzL"
      },
      "source": [
        "y_test_dict = {0: [], 1: [], 2: [],3: [],4: [],5: [],6: [],7: [],8: [],9:[]}\n",
        "\n",
        "#initialize X_Test matrix via an array (2-D)\n",
        "X_test = np.array(dict_test_features[0])\n",
        "y_test_dict[0] = np.zeros((np.array(dict_test_features[0]).shape[0],))\n",
        "y_test_mc = np.array(y_test_dict[0])\n",
        "\n",
        "#iteratively concatenate the coloumns relative to the other digits\n",
        "for i in np.arange(1, Len_digit):\n",
        "    X_test = np.concatenate((X_test, dict_test_features[i]), axis = 0)\n",
        "    y_test_dict[i] = np.ones((np.array(dict_test_features[i]).shape[0],))*i\n",
        "    y_test_mc = np.concatenate((y_test_mc, y_test_dict[i]), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M3jxMSmqnw4"
      },
      "source": [
        "Here we build a 3D ordered matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5hfBylfqZKg"
      },
      "source": [
        "## X_train_matrix method => we build a 3D matrix\n",
        "X_train_matrix = np.zeros((Len_digit,Len_train,n_mfcc));\n",
        "for i in np.arange(Len_digit):\n",
        "  X_train_matrix[i] = dict_train_features[i];\n",
        "\n",
        "X_test_matrix = np.zeros((Len_digit,Len_test,n_mfcc));\n",
        "for i in np.arange(Len_digit):\n",
        "  X_test_matrix[i] = dict_test_features[i];\n",
        "\n",
        "y_train_matrix = np.ones((Len_digit,Len_train))\n",
        "y_test_matrix = np.ones((Len_digit,Len_test))  \n",
        "\n",
        "for i in np.arange(Len_digit):\n",
        "  y_train_matrix[i] = y_train_matrix[i]*i;\n",
        "  y_test_matrix[i] = y_test_matrix[i]*i;\n",
        "\n",
        "y_train_matrix_conc = y_train_matrix[0]\n",
        "for i in np.arange(1,Len_digit):\n",
        "  y_train_matrix_conc = np.concatenate((y_train_matrix_conc,y_train_matrix[i]),axis=0)\n",
        "\n",
        "y_test_matrix_conc = y_test_matrix[0]\n",
        "for i in np.arange(1,Len_digit):\n",
        "  y_test_matrix_conc = np.concatenate((y_test_matrix_conc,y_test_matrix[i]),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJL31DXpYU0l"
      },
      "source": [
        "Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBOT8be69Y9K"
      },
      "source": [
        "feat_max = np.max(X_train, axis=0)\n",
        "feat_min = np.min(X_train, axis=0)\n",
        "\n",
        "X_train_matrix_normalized = np.zeros((Len_digit,Len_train,n_mfcc));\n",
        "X_test_matrix_normalized = np.zeros((Len_digit,Len_test,n_mfcc));\n",
        "for i in np.arange(Len_digit):\n",
        "  X_train_matrix_normalized[i] = (X_train_matrix[i] - feat_min) / (feat_max - feat_min);\n",
        "  X_test_matrix_normalized[i] = (X_test_matrix[i] - feat_min) / (feat_max - feat_min);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS8GUsUJYXKe"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oT-Iv-3HF1f"
      },
      "source": [
        "SVM_parameters = {\n",
        "    'C': 1,\n",
        "    'kernel': 'rbf'\n",
        "}\n",
        "\n",
        "Len_comb =int(0.5*Len_digit*(Len_digit-1));\n",
        "clf_vec = np.empty((0,0), dtype=sklearn.svm._classes.SVC);\n",
        "\n",
        "for i in np.arange(Len_comb):\n",
        "  clf_vec = np.append(clf_vec, sklearn.svm.SVC(**SVM_parameters, probability=True));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvAw3wC9fbIC"
      },
      "source": [
        "Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5ppCbJ_YnXj"
      },
      "source": [
        "indx_clf = 0;\n",
        "#we perform a cycle over all the possible permutations\n",
        "for i in np.arange(Len_digit-1):\n",
        "  for j in np.arange(i+1,Len_digit):\n",
        "    #fit the clf_vec[indx_clf] to the corresponding digits\n",
        "    clf_vec[indx_clf].fit(np.concatenate((X_train_matrix_normalized[i],X_train_matrix_normalized[j]), axis=0),np.concatenate((y_train_matrix[i], y_train_matrix[j]), axis=0));\n",
        "    indx_clf +=1;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hl1lf3wfdNn"
      },
      "source": [
        "Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3s50tJrYv8u"
      },
      "source": [
        "#concatenate all the normalized test features into one array\n",
        "X_test_mc_normalized = X_test_matrix_normalized[0];\n",
        "for i in np.arange(1,Len_digit):\n",
        "  X_test_mc_normalized = np.concatenate((X_test_mc_normalized,X_test_matrix_normalized[i]),axis=0);\n",
        "\n",
        "#prepare an array to contain 300 predictions (the total number of test files) for each binary classifier\n",
        "y_test_predict = np.zeros((Len_comb,X_test_mc_normalized.shape[0],1))\n",
        "\n",
        "#fill the array of predictions, one for each combination of digits\n",
        "for i in np.arange(Len_comb):\n",
        "  y_test_predict[i] = clf_vec[i].predict(X_test_mc_normalized).reshape(-1, 1);\n",
        "\n",
        "#concatenate all the predictions into one array\n",
        "y_test_predicted_mc = y_test_predict[0];\n",
        "for i in np.arange(1,Len_comb):\n",
        "  y_test_predicted_mc = np.concatenate((y_test_predicted_mc,y_test_predict[i]),axis=1);\n",
        "\n",
        "#convert the predictions into integer types\n",
        "y_test_predicted_mc = np.array(y_test_predicted_mc, dtype=np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5_xzyopfWIT"
      },
      "source": [
        "Majority Voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8OqksNcfVTw"
      },
      "source": [
        "y_test_predicted_mv = np.zeros((y_test_predicted_mc.shape[0],))\n",
        "\n",
        "#count and select the most predicted digit for each file\n",
        "for i, e in enumerate(y_test_predicted_mc):\n",
        "    y_test_predicted_mv[i] = np.bincount(e).argmax()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LynZ_nH5hG1M"
      },
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MScnb24EhGMc"
      },
      "source": [
        "#function to compute the confusion matrix\n",
        "def compute_cm_multiclass(gt, predicted):\n",
        "    classes = np.unique(gt)\n",
        "    \n",
        "    CM = np.zeros((len(classes), len(classes)))\n",
        "    print('    0   1   2   3   4   5   6   7   8   9')\n",
        "    for i in np.arange(len(classes)):\n",
        "        #select the predictions for each class\n",
        "        pred_class = predicted[gt==i]\n",
        "\n",
        "        #put the predictions in the matrix \n",
        "        #(row index = correct class, column index = predicted class)\n",
        "        for j in np.arange(len(pred_class)):\n",
        "            CM[i, int(pred_class[j])] = CM[i, int(pred_class[j])] + 1\n",
        "        print(i, CM[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X--UiugZZ9TW",
        "outputId": "1ee02eca-b471-436c-a4c6-66102b46d352"
      },
      "source": [
        "cm = compute_cm_multiclass(y_test_matrix_conc, y_test_predicted_mv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0   1   2   3   4   5   6   7   8   9\n",
            "0 [29.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
            "1 [ 0. 29.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
            "2 [ 0.  0. 30.  0.  0.  0.  0.  0.  0.  0.]\n",
            "3 [ 0.  0.  2. 26.  0.  0.  2.  0.  0.  0.]\n",
            "4 [ 0.  0.  0.  0. 30.  0.  0.  0.  0.  0.]\n",
            "5 [ 1.  0.  0.  0.  0. 29.  0.  0.  0.  0.]\n",
            "6 [ 0.  0.  0.  2.  0.  0. 26.  1.  1.  0.]\n",
            "7 [ 0.  0.  0.  0.  0.  0.  0. 30.  0.  0.]\n",
            "8 [ 0.  0.  0.  1.  0.  0.  2.  0. 27.  0.]\n",
            "9 [ 0.  2.  0.  0.  0.  0.  0.  0.  0. 28.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DywDqWP48CJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a69b2e-14de-4e85-f70f-1b1eeb9d8ef1"
      },
      "source": [
        "print(metrics.classification_report(y_test_matrix_conc, y_test_predicted_mv, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.967     0.967     0.967        30\n",
            "         1.0      0.935     0.967     0.951        30\n",
            "         2.0      0.938     1.000     0.968        30\n",
            "         3.0      0.867     0.867     0.867        30\n",
            "         4.0      1.000     1.000     1.000        30\n",
            "         5.0      1.000     0.967     0.983        30\n",
            "         6.0      0.867     0.867     0.867        30\n",
            "         7.0      0.968     1.000     0.984        30\n",
            "         8.0      0.964     0.900     0.931        30\n",
            "         9.0      0.966     0.933     0.949        30\n",
            "\n",
            "    accuracy                          0.947       300\n",
            "   macro avg      0.947     0.947     0.947       300\n",
            "weighted avg      0.947     0.947     0.947       300\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cngETv9foFp"
      },
      "source": [
        "# OneVsRest SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FME8Lp6I8H6z"
      },
      "source": [
        "#concatenate the training features\n",
        "X_train_mc_normalized = X_train_matrix_normalized[0];\n",
        "for i in np.arange(1,Len_digit):\n",
        "  X_train_mc_normalized = np.concatenate((X_train_mc_normalized,X_train_matrix_normalized[i]),axis=0);\n",
        "\n",
        "#fit the model\n",
        "clf = OneVsRestClassifier(SVC()).fit(X_train_mc_normalized, y_train_matrix_conc)\n",
        "\n",
        "#predicting the results\n",
        "y_test_predicted_ovr = clf.predict(X_test_mc_normalized)\n",
        "\n",
        "#convert the predictions into integer types\n",
        "y_test_predicted_ovr = np.array(y_test_predicted_ovr, dtype=np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukxpmM0ao9Ah"
      },
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQcLL-AH8H3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8d3cea-b38f-47c3-f131-be124a179934"
      },
      "source": [
        "cm2 = compute_cm_multiclass(y_test_matrix_conc, y_test_predicted_ovr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0   1   2   3   4   5   6   7   8   9\n",
            "0 [29.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
            "1 [ 0. 29.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
            "2 [ 0.  0. 30.  0.  0.  0.  0.  0.  0.  0.]\n",
            "3 [ 0.  0.  3. 25.  0.  0.  2.  0.  0.  0.]\n",
            "4 [ 0.  0.  0.  0. 30.  0.  0.  0.  0.  0.]\n",
            "5 [ 1.  0.  0.  0.  0. 29.  0.  0.  0.  0.]\n",
            "6 [ 1.  0.  0.  2.  0.  0. 25.  1.  1.  0.]\n",
            "7 [ 0.  0.  0.  0.  0.  0.  0. 30.  0.  0.]\n",
            "8 [ 0.  0.  0.  0.  0.  0.  1.  0. 29.  0.]\n",
            "9 [ 1.  2.  0.  0.  0.  0.  0.  1.  0. 26.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXb6OzRr8YG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b135f293-88cd-4fce-c56d-4bf3ae6792b2"
      },
      "source": [
        "print(metrics.classification_report(y_test_matrix_conc, y_test_predicted_ovr, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.906     0.967     0.935        30\n",
            "         1.0      0.935     0.967     0.951        30\n",
            "         2.0      0.909     1.000     0.952        30\n",
            "         3.0      0.893     0.833     0.862        30\n",
            "         4.0      1.000     1.000     1.000        30\n",
            "         5.0      1.000     0.967     0.983        30\n",
            "         6.0      0.893     0.833     0.862        30\n",
            "         7.0      0.938     1.000     0.968        30\n",
            "         8.0      0.967     0.967     0.967        30\n",
            "         9.0      0.963     0.867     0.912        30\n",
            "\n",
            "    accuracy                          0.940       300\n",
            "   macro avg      0.940     0.940     0.939       300\n",
            "weighted avg      0.940     0.940     0.939       300\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFxT6PMn8tY1"
      },
      "source": [
        "##Our Files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXIiBA1j8vi_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27576cb6-2846-4730-fa5f-8f0f89b9d155"
      },
      "source": [
        "! git clone https://github.com/FrancescoBorgna/CMLS_HW1_VoiceSpokenDigits\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'CMLS_HW1_VoiceSpokenDigits' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL8X3zEojNBg"
      },
      "source": [
        "test_root = ('CMLS_HW1_VoiceSpokenDigits/Audio')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bGdGkGUncxi"
      },
      "source": [
        "Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRmS17v6j4xW"
      },
      "source": [
        "def test_predictor(test_root):\n",
        "  dict_test_features = {0: [], 1: [], 2: [],3: [],4: [],5: [],6: [],7: [],8: [],9:[]}\n",
        "\n",
        "  class_test_files = [f for f in os.listdir(test_root) if f.endswith('.wav')]\n",
        "\n",
        "  for i in np.arange(len(class_test_files)):\n",
        "    tmp = class_test_files[i];\n",
        "    tmp = (tmp.split('.'))[0].split('_');\n",
        "  \n",
        "    audio, fs = librosa.load(os.path.join(test_root,class_test_files[i]), sr=None)\n",
        "    mfcc = compute_mfcc(audio, fs, n_mfcc)\n",
        "    tmp_features = np.mean(mfcc, axis=1);\n",
        "  \n",
        "    dict_test_features[int(tmp[0])].append(tmp_features)\n",
        "\n",
        "\n",
        "  Len_test = len(dict_test_features[0])\n",
        "  Len_digit = 10;\n",
        "\n",
        "  #SVM\n",
        "  y_test_dict = {0: [], 1: [], 2: [],3: [],4: [],5: [],6: [],7: [],8: [],9:[]}\n",
        "\n",
        "  X_test = np.array(dict_test_features[0])\n",
        "  y_test_dict[0] = np.zeros((np.array(dict_test_features[0]).shape[0],))\n",
        "  y_test_mc = np.array(y_test_dict[0])\n",
        "\n",
        "  for i in np.arange(1, Len_digit):\n",
        "    X_test = np.concatenate((X_test, dict_test_features[i]), axis = 0)\n",
        "    y_test_dict[i] = np.ones((np.array(dict_test_features[i]).shape[0],))*i\n",
        "    y_test_mc = np.concatenate((y_test_mc, y_test_dict[i]), axis = 0)\n",
        "\n",
        "  ## X_train_matrix method\n",
        "  X_test_matrix = np.zeros((Len_digit,Len_test,n_mfcc));\n",
        "  for i in np.arange(Len_digit):\n",
        "    X_test_matrix[i] = dict_test_features[i];\n",
        "\n",
        "\n",
        "  y_test_matrix = np.ones((Len_digit,Len_test))  \n",
        "\n",
        "  for i in np.arange(Len_digit):\n",
        "    y_test_matrix[i] = y_test_matrix[i]*i;\n",
        "\n",
        "  y_test_matrix_conc = y_test_matrix[0]\n",
        "  for i in np.arange(1,Len_digit):\n",
        "    y_test_matrix_conc = np.concatenate((y_test_matrix_conc,y_test_matrix[i]),axis=0)\n",
        "\n",
        "\n",
        "  #Normalization\n",
        "  X_test_matrix_normalized = np.zeros((Len_digit,Len_test,n_mfcc));\n",
        "  for i in np.arange(Len_digit):\n",
        "    X_test_matrix_normalized[i] = (X_test_matrix[i] - feat_min) / (feat_max - feat_min);\n",
        "\n",
        "  #concatenate all the normalized test features into one array\n",
        "  X_test_mc_normalized = X_test_matrix_normalized[0];\n",
        "  for i in np.arange(1,Len_digit):\n",
        "    X_test_mc_normalized = np.concatenate((X_test_mc_normalized,X_test_matrix_normalized[i]),axis=0);\n",
        "\n",
        "  #Predict\n",
        "  #prepare an array to contain 40 predictions (the total number of test files) for each binary classifier\n",
        "  y_test_predict = np.zeros((Len_comb,X_test_mc_normalized.shape[0],1))\n",
        "\n",
        "  #fill the array of predictions, one for each combination of digits\n",
        "  for i in np.arange(Len_comb):\n",
        "    y_test_predict[i] = clf_vec[i].predict(X_test_mc_normalized).reshape(-1, 1);\n",
        "\n",
        "  #concatenate all the predictions into one array\n",
        "  y_test_predicted_mc = y_test_predict[0];\n",
        "  for i in np.arange(1,Len_comb):\n",
        "    y_test_predicted_mc = np.concatenate((y_test_predicted_mc,y_test_predict[i]),axis=1);\n",
        "\n",
        "  #convert the predictions into integer types\n",
        "  y_test_predicted_mc = np.array(y_test_predicted_mc, dtype=np.int)\n",
        "  \n",
        "  #Majority voting\n",
        "\n",
        "  y_test_predicted_mv = np.zeros((y_test_predicted_mc.shape[0],))\n",
        "\n",
        "  #count and select the most predicted digit for each file\n",
        "  for i, e in enumerate(y_test_predicted_mc):\n",
        "    y_test_predicted_mv[i] = np.bincount(e).argmax() \n",
        "  print(metrics.classification_report(y_test_matrix_conc, y_test_predicted_mv, digits=3)) \n",
        "  return compute_cm_multiclass(y_test_matrix_conc, y_test_predicted_mv);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aLZupkUm6mf",
        "outputId": "af416e40-5af0-4d7e-b9d2-4e2fe5941767"
      },
      "source": [
        "test_predictor(test_root)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.000     0.000     0.000         4\n",
            "         1.0      0.600     0.750     0.667         4\n",
            "         2.0      0.400     0.500     0.444         4\n",
            "         3.0      0.222     0.500     0.308         4\n",
            "         4.0      1.000     0.250     0.400         4\n",
            "         5.0      1.000     0.750     0.857         4\n",
            "         6.0      1.000     0.250     0.400         4\n",
            "         7.0      0.000     0.000     0.000         4\n",
            "         8.0      0.400     0.500     0.444         4\n",
            "         9.0      0.333     0.750     0.462         4\n",
            "\n",
            "    accuracy                          0.425        40\n",
            "   macro avg      0.496     0.425     0.398        40\n",
            "weighted avg      0.496     0.425     0.398        40\n",
            "\n",
            "    0   1   2   3   4   5   6   7   8   9\n",
            "0 [0. 0. 1. 1. 0. 0. 0. 0. 1. 1.]\n",
            "1 [0. 3. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "2 [0. 0. 2. 2. 0. 0. 0. 0. 0. 0.]\n",
            "3 [0. 0. 1. 2. 0. 0. 0. 0. 1. 0.]\n",
            "4 [1. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            "5 [0. 0. 0. 0. 0. 3. 0. 0. 0. 1.]\n",
            "6 [0. 0. 0. 2. 0. 0. 1. 1. 0. 0.]\n",
            "7 [0. 0. 0. 0. 0. 0. 0. 0. 1. 3.]\n",
            "8 [0. 0. 0. 2. 0. 0. 0. 0. 2. 0.]\n",
            "9 [0. 1. 0. 0. 0. 0. 0. 0. 0. 3.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}